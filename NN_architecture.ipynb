{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_architecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQ/LTltAt9m3SC/o+m1vpA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halilyaman/neural_network_implementation/blob/master/NN_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXe8pRunGaiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXSoSODaupiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Activations:\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid(X):\n",
        "\n",
        "    return 1 / (1 + np.exp(-X))\n",
        "\n",
        "  @staticmethod\n",
        "  def linear(X):\n",
        "\n",
        "    return X\n",
        "\n",
        "  @staticmethod\n",
        "  def relu(X):\n",
        "\n",
        "    return np.maximum(0, X)\n",
        "\n",
        "  @staticmethod\n",
        "  def leaky_relu(X):\n",
        "\n",
        "    return np.maximum(0.01 * X, X)\n",
        "\n",
        "  @staticmethod\n",
        "  def tanh(X):\n",
        "\n",
        "    return np.tanh(X)\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid_derivative(X):\n",
        "\n",
        "    X_sigmoid = Activations.sigmoid(X)\n",
        "\n",
        "    return X_sigmoid * (1 - X_sigmoid)\n",
        "\n",
        "  @staticmethod\n",
        "  def relu_derivative(X):\n",
        "    \n",
        "    derivatives = np.zeros(X.shape)\n",
        "\n",
        "    for i, rows in enumerate(X):\n",
        "\n",
        "      for j, v in enumerate(rows):\n",
        "\n",
        "        if v < 0:\n",
        "          derivatives[i, j] = 0\n",
        "        else:\n",
        "          derivatives[i, j] = 1\n",
        "\n",
        "    return derivatives\n",
        "\n",
        "  @staticmethod\n",
        "  def leaky_relu_derivative(X):\n",
        "    \n",
        "    derivatives = np.zeros(X.shape)\n",
        "\n",
        "    for i, rows in enumerate(X):\n",
        "\n",
        "      for j, v in enumerate(rows):\n",
        "\n",
        "        if v < 0:\n",
        "          derivatives[i, j] = 0.01\n",
        "        else:\n",
        "          derivatives[i, j] = 1\n",
        "\n",
        "    return derivatives\n",
        "\n",
        "  @staticmethod\n",
        "  def tanh_derivative(X):\n",
        "\n",
        "    tanh = Activations.tanh(X)\n",
        "    \n",
        "    return 1 - tanh ** 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW01W026GtId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer:\n",
        "\n",
        "  def __init__(self, units, input_size=0, activation=\"linear\"):\n",
        "\n",
        "    self.units = units\n",
        "    self.input_size = input_size\n",
        "    self.activation = activation\n",
        "    self.layer_type = None\n",
        "    self.predictions = None\n",
        "    self.inputs = None\n",
        "    self.learning_rate = None\n",
        "    self.next_layer = None\n",
        "    self.activation_derivative = None\n",
        "    self.z_1 = None\n",
        "\n",
        "    if input_size > 0:\n",
        "      self.__init_weights()\n",
        "  \n",
        "  def __init_weights(self,):\n",
        "\n",
        "    self.w = np.random.rand(self.units, self.input_size)\n",
        "    self.b = np.zeros((self.units, 1))\n",
        "\n",
        "  def _forward_prop(self, X):\n",
        "\n",
        "    if X.shape[1] != self.input_size:\n",
        "      raise Exception(\"input shape doesn't match with the data!\")\n",
        "\n",
        "    self.inputs = X\n",
        "\n",
        "    dot_products = np.dot(self.w, X.T)\n",
        "    pred = dot_products + self.b\n",
        "    self.z_1 = pred\n",
        "    pred = self._choose_activation(pred)\n",
        "\n",
        "    self.predictions = pred\n",
        "\n",
        "    return self.predictions.T\n",
        "  \n",
        "  def _backward_prop(self, Y):\n",
        "\n",
        "    avg_factor = (1 / len(self.inputs))\n",
        "\n",
        "    # derivative calculations are different in order to layer type.\n",
        "    # So we need these conditions\n",
        "    if self.layer_type == \"output_layer\":\n",
        "\n",
        "      # calculating new weights and b values\n",
        "      self.d_z = self.predictions - Y\n",
        "      d_w = avg_factor * self.d_z.dot(self.inputs)\n",
        "      d_b = avg_factor * np.sum(self.d_z, axis=1, keepdims=True)\n",
        "      \n",
        "      # updating weights and b\n",
        "      self.b = self.b - self.learning_rate * d_b\n",
        "      self.w = self.w - self.learning_rate * d_w\n",
        "    \n",
        "    if self.layer_type == \"hidden_layer\":\n",
        "\n",
        "      # calculating new weights and b values\n",
        "      self.d_z = self.next_layer.w.T.dot(self.next_layer.d_z) * self.activation_derivative(self.z_1)\n",
        "      d_w = avg_factor * self.d_z.dot(self.inputs)\n",
        "      d_b = avg_factor * np.sum(self.d_z, axis=1, keepdims=True)\n",
        "\n",
        "      # updating weights and b\n",
        "      self.b = self.b - self.learning_rate * d_b\n",
        "      self.w = self.w - self.learning_rate * d_w\n",
        "\n",
        "\n",
        "  def _choose_activation(self, X):\n",
        "\n",
        "    if self.activation == \"sigmoid\":\n",
        "      X = Activations.sigmoid(X)\n",
        "      self.activation_derivative = Activations.sigmoid_derivative\n",
        "    \n",
        "    elif self.activation == \"linear\":\n",
        "      X = Activations.linear(X)\n",
        "    \n",
        "    elif self.activation == \"relu\":\n",
        "      X = Activations.relu(X)\n",
        "      self.activation_derivative = Activations.relu_derivative\n",
        "\n",
        "    elif self.activation == \"leaky_relu\":\n",
        "      X = Activations.leaky_relu(X)\n",
        "      self.activation_derivative = Activations.leaky_relu_derivative\n",
        "\n",
        "    elif self.activation == \"tanh\":\n",
        "      X = Activations.tanh(X)\n",
        "      self.activation_derivative = Activations.tanh_derivative\n",
        "\n",
        "    return X\n",
        "    \n",
        "  def _bind_to(self, layer):\n",
        "\n",
        "    self.input_size = layer.units\n",
        "    self.__init_weights()\n",
        "\n",
        "  def _set_layer_type(self, layer_type):\n",
        "\n",
        "    self.layer_type = layer_type\n",
        "\n",
        "  def _set_learning_rate(self, learning_rate):\n",
        "    \n",
        "    self.learning_rate = learning_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKi5ojI9zXl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel:\n",
        "  \n",
        "  def __init__(self, learning_rate=0.01):\n",
        "\n",
        "    self.layers = list()\n",
        "    self.learning_rate = learning_rate\n",
        "    self.history = dict()\n",
        "\n",
        "\n",
        "  def add_layer(self, layer):\n",
        "\n",
        "    if len(self.layers) == 0:\n",
        "\n",
        "      self.layers.append(layer)\n",
        "      self.layers[0]._set_layer_type(\"output_layer\")\n",
        "\n",
        "    else:\n",
        "\n",
        "      layer._bind_to(self.layers[-1])\n",
        "      self.layers[-1].next_layer = layer\n",
        "      self.layers.append(layer)\n",
        "      \n",
        "\n",
        "      n = len(self.layers)\n",
        "      last_layer_i = n - 1\n",
        "      before_last_i = n - 2\n",
        "\n",
        "      self.layers[last_layer_i]._set_layer_type(\"output_layer\")\n",
        "      self.layers[before_last_i]._set_layer_type(\"hidden_layer\")\n",
        "\n",
        "    self.layers[-1]._set_learning_rate(self.learning_rate)\n",
        "\n",
        "\n",
        "  def fit(self, X, Y, epochs, verbose=True):\n",
        "\n",
        "    self.history[\"loss\"] = []\n",
        "    m = len(Y)\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "      if verbose:\n",
        "        print(\"Epoch {}\\n\".format(i+1))\n",
        "  \n",
        "      current_output = X\n",
        "      for j in self.layers:\n",
        "\n",
        "        current_output = j._forward_prop(current_output)\n",
        "      \n",
        "      for j in reversed(range(len(self.layers))):\n",
        "\n",
        "        self.layers[j]._backward_prop(Y)\n",
        "      \n",
        "      logprobs = np.multiply(np.log(current_output.flatten()), Y) + np.multiply(np.log(1-current_output.flatten()), (1-Y))\n",
        "      cost = - np.sum(logprobs) / m\n",
        "      \n",
        "      self.history[\"loss\"].append(cost)\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "\n",
        "    current_output = X\n",
        "    for i in self.layers:\n",
        "\n",
        "      current_output = i._forward_prop(current_output)\n",
        "    \n",
        "    return current_output\n",
        "\n",
        "  \n",
        "  def print_layers(self,):\n",
        "    print(\"\\n****************************\\n\")\n",
        "\n",
        "    for i, v in enumerate(self.layers):\n",
        "      print(\"Layer Index: {}\\nLayer Type: {}\\nUnits: {}\\nActivation: {}\\n\"\n",
        "      .format(i, v.layer_type, v.units, v.activation, v.input_size))\n",
        "    \n",
        "    print(\"****************************\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niVW_wLVQ1nj",
        "colab_type": "code",
        "outputId": "5481df27-61bb-4d6d-9875-85b72c05f133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "X = np.array([[1, 1], [1, 0], [0, 1], [0, 0]])\n",
        "Y = np.array([0, 1, 1, 0])\n",
        "\n",
        "past = time.time()\n",
        "\n",
        "model = MyModel(0.1)\n",
        "\n",
        "layer_1 = Layer(units=4, input_size=2, activation=\"tanh\")\n",
        "layer_2 = Layer(units=1, activation=\"sigmoid\")\n",
        "\n",
        "model.add_layer(layer_1)\n",
        "model.add_layer(layer_2)\n",
        "\n",
        "before_fit = model.predict(X)\n",
        "model.fit(X, Y, 2000, verbose=False)\n",
        "after_fit = model.predict(X)\n",
        "\n",
        "print(\"Results before fitting:\\n\", before_fit)\n",
        "print(\"Results after fitting:\\n\", after_fit)\n",
        "print(\"\\nTime passed: \", time.time() - past)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results before fitting:\n",
            " [[0.76046886]\n",
            " [0.69458361]\n",
            " [0.6075779 ]\n",
            " [0.5       ]]\n",
            "Results after fitting:\n",
            " [[0.03052927]\n",
            " [0.97662765]\n",
            " [0.98004212]\n",
            " [0.00624055]]\n",
            "\n",
            "Time passed:  0.15421843528747559\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIPH6QVKREPG",
        "colab_type": "code",
        "outputId": "91cc9f46-e803-4dd6-bafe-95ff2880d8a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.plot(model.history[\"loss\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f64cd6f3ac8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MrC_96GCF82",
        "colab_type": "code",
        "outputId": "114228dd-09e9-4a23-f885-5a01d9a345e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "X = np.array([[1, 1], [1, 0], [0, 1], [0, 0]])\n",
        "Y = np.array([0, 1, 1, 0])\n",
        "\n",
        "unit_numbers = [1, 2, 3, 4, 5, 10, 20, 50, 100, 9999]\n",
        "\n",
        "for n_hidden_units in unit_numbers:\n",
        "\n",
        "  model = MyModel(0.1)\n",
        "\n",
        "  layer_1 = Layer(units=n_hidden_units, input_size=2, activation=\"tanh\")\n",
        "  layer_2 = Layer(units=1, activation=\"sigmoid\")\n",
        "\n",
        "  model.add_layer(layer_1)\n",
        "  model.add_layer(layer_2)\n",
        "\n",
        "  model.fit(X, Y, epochs=1000, verbose=False)\n",
        "\n",
        "  predictions = model.predict(X)\n",
        "\n",
        "  accuracy = float((np.dot(Y,predictions) + np.dot(1-Y,1-predictions))/float(Y.size)*100)\n",
        "\n",
        "  print(\"Accuracy for {} hidden units: {}%\".format(n_hidden_units, accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for 1 hidden units: 63.00705443475494%\n",
            "Accuracy for 2 hidden units: 84.88186855926347%\n",
            "Accuracy for 3 hidden units: 89.04218512699994%\n",
            "Accuracy for 4 hidden units: 89.64235026378331%\n",
            "Accuracy for 5 hidden units: 87.54838609894185%\n",
            "Accuracy for 10 hidden units: 95.78827219177471%\n",
            "Accuracy for 20 hidden units: 96.79291917675019%\n",
            "Accuracy for 50 hidden units: 97.87297320386298%\n",
            "Accuracy for 100 hidden units: 98.22795791826572%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy for 9999 hidden units: 99.99973940696151%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia7vsOBsjWfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}