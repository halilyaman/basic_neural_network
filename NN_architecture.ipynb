{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_architecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOM0R8xcAvLsa9HQ1CM6EUx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halilyaman/neural_network_implementation/blob/master/NN_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1WJ5oHSamXo",
        "colab_type": "text"
      },
      "source": [
        "#### **Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXe8pRunGaiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDKslBsRaxmw",
        "colab_type": "text"
      },
      "source": [
        "# **Activations class**\n",
        "This class contains activation functions and derivatives of them. All functions are static in order to use them directly in a neural network. \\\n",
        "X represents the input matrix. \\\n",
        "\n",
        "---\n",
        " **sigmoid** \\\n",
        "![Sigmoid Function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/400px-Logistic-curve.svg.png)\n",
        "\n",
        "---\n",
        "**tanh** \\\n",
        "![Tanh Function](https://www.medcalc.org/manual/_help/functions/tanh.png)\n",
        "\n",
        "---\n",
        "**ReLU** \\\n",
        "![ReLU Function](https://miro.medium.com/max/400/0*g9ypL5M3k-f7EW85.png)\n",
        "\n",
        "---\n",
        "**Leaky Relu** \\\n",
        "![Leaky ReLU](https://1.bp.blogspot.com/-5ymhxBydo8A/XPj_qXK-sWI/AAAAAAAABU4/UjgZ7eChpwsoPa1_bZjvdrzKCsCfQPaJgCLcBGAs/s400/leaking_relu_2.PNG)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXSoSODaupiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Activations:\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid(X):\n",
        "\n",
        "    return 1 / (1 + np.exp(-X))\n",
        "\n",
        "  @staticmethod\n",
        "  def linear(X):\n",
        "\n",
        "    return X\n",
        "\n",
        "  @staticmethod\n",
        "  def relu(X):\n",
        "\n",
        "    return np.maximum(0, X)\n",
        "\n",
        "  @staticmethod\n",
        "  def leaky_relu(X):\n",
        "\n",
        "    return np.maximum(0.01 * X, X)\n",
        "\n",
        "  @staticmethod\n",
        "  def tanh(X):\n",
        "\n",
        "    return np.tanh(X)\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid_derivative(X):\n",
        "\n",
        "    X_sigmoid = Activations.sigmoid(X)\n",
        "\n",
        "    return X_sigmoid * (1 - X_sigmoid)\n",
        "\n",
        "  @staticmethod\n",
        "  def relu_derivative(X):\n",
        "    \n",
        "    derivatives = np.zeros(X.shape)\n",
        "\n",
        "    for i, rows in enumerate(X):\n",
        "\n",
        "      for j, v in enumerate(rows):\n",
        "\n",
        "        if v < 0:\n",
        "          derivatives[i, j] = 0\n",
        "        else:\n",
        "          derivatives[i, j] = 1\n",
        "\n",
        "    return derivatives\n",
        "\n",
        "  @staticmethod\n",
        "  def leaky_relu_derivative(X):\n",
        "    \n",
        "    derivatives = np.zeros(X.shape)\n",
        "\n",
        "    for i, rows in enumerate(X):\n",
        "\n",
        "      for j, v in enumerate(rows):\n",
        "\n",
        "        if v < 0:\n",
        "          derivatives[i, j] = 0.01\n",
        "        else:\n",
        "          derivatives[i, j] = 1\n",
        "\n",
        "    return derivatives\n",
        "\n",
        "  @staticmethod\n",
        "  def tanh_derivative(X):\n",
        "\n",
        "    tanh = Activations.tanh(X)\n",
        "    \n",
        "    return 1 - tanh ** 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1xfiWQjcLJ_",
        "colab_type": "text"
      },
      "source": [
        "# **Layer class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW01W026GtId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer:\n",
        "\n",
        "  \"\"\"\n",
        "  @Author: Halil Yaman\n",
        "\n",
        "  This class is used for creating a layer for our neural network model.\n",
        "  It is only capable of doing binary classification for now. But others will\n",
        "  be added soon.\n",
        "  \n",
        "  Each new layer comes one after another. You can use the 'add' function in\n",
        "  Model class for adding new layer to another one.\n",
        "\n",
        "  There are two types of layers, hidden layer and output layer.\n",
        "  In the first layer, you must specify the input_shape. For other layers,\n",
        "  you don't need to specify it. It is done automatically.\n",
        "\n",
        "  The number of units in output layer must match with the number of elements \n",
        "  in your one output sample.\n",
        "\n",
        "  All calculations for back and forward propagations are vectorized by using\n",
        "  numpy library. So, efficiency is improved by utilizing the multiple cores.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, units, input_shape=None, activation=\"linear\"):\n",
        "\n",
        "    \"\"\"\n",
        "    :param units: Dimension of the output space\n",
        "    :param input_shape: Shape of the one single input\n",
        "    :param activation: Activation function used in this specific layer.\n",
        "    \"\"\"\n",
        "\n",
        "    self.units = units\n",
        "    self.input_shape = input_shape\n",
        "    self.activation = activation\n",
        "    self.layer_type = None\n",
        "    self.predictions = None\n",
        "    self.inputs = None\n",
        "    self.outputs = None\n",
        "    self.learning_rate = None\n",
        "    self.next_layer = None\n",
        "    self.activation_derivative = None\n",
        "    self.z = None\n",
        "\n",
        "    if not(self.input_shape == None):\n",
        "      self.__init_weights()\n",
        "  \n",
        "  def __init_weights(self,):\n",
        "\n",
        "    self.w = np.random.rand(self.units, self.input_shape[0])\n",
        "    self.b = np.zeros((self.units, 1))\n",
        "\n",
        "  def _forward_prop(self, X):\n",
        "\n",
        "    if X.shape[0] != self.input_shape[0]:\n",
        "      raise Exception(\"input shape doesn't match with the data!\")\n",
        "\n",
        "    self.inputs = X\n",
        "\n",
        "    dot_products = np.dot(self.w, X)\n",
        "    self.z = dot_products + self.b\n",
        "\n",
        "    self.outputs = self._choose_activation(self.z)\n",
        "\n",
        "    return self.outputs\n",
        "  \n",
        "  def _backward_prop(self, da):\n",
        "\n",
        "    avg_factor = (1 / len(self.inputs))\n",
        "\n",
        "    # calculating new weights and bias\n",
        "    self.d_z = da * self.activation_derivative(self.z)\n",
        "    \n",
        "    # calculate new weight and bias values\n",
        "    d_w = avg_factor * self.d_z.dot(self.inputs.T)\n",
        "    d_b = avg_factor * np.sum(self.d_z, axis=1, keepdims=True)\n",
        "\n",
        "    # updating weights and b\n",
        "    self.b = self.b - self.learning_rate * d_b\n",
        "    self.w = self.w - self.learning_rate * d_w\n",
        "\n",
        "    da = self.w.T.dot(self.d_z)\n",
        "\n",
        "    return da\n",
        "\n",
        "\n",
        "  def _choose_activation(self, X):\n",
        "\n",
        "    if self.activation == \"sigmoid\":\n",
        "      X = Activations.sigmoid(X)\n",
        "      self.activation_derivative = Activations.sigmoid_derivative\n",
        "    \n",
        "    elif self.activation == \"linear\":\n",
        "      X = Activations.linear(X)\n",
        "    \n",
        "    elif self.activation == \"relu\":\n",
        "      X = Activations.relu(X)\n",
        "      self.activation_derivative = Activations.relu_derivative\n",
        "\n",
        "    elif self.activation == \"leaky_relu\":\n",
        "      X = Activations.leaky_relu(X)\n",
        "      self.activation_derivative = Activations.leaky_relu_derivative\n",
        "\n",
        "    elif self.activation == \"tanh\":\n",
        "      X = Activations.tanh(X)\n",
        "      self.activation_derivative = Activations.tanh_derivative\n",
        "\n",
        "    return X\n",
        "    \n",
        "  def _bind_to(self, layer):\n",
        "\n",
        "    self.input_shape = (layer.units, 1)\n",
        "    self.__init_weights()\n",
        "\n",
        "  def _set_layer_type(self, layer_type):\n",
        "\n",
        "    self.layer_type = layer_type\n",
        "\n",
        "  def _set_learning_rate(self, learning_rate):\n",
        "    \n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "  def _set_next_layer(self, next_layer):\n",
        "\n",
        "    self.next_layer = next_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rD9tp5FcdwM",
        "colab_type": "text"
      },
      "source": [
        "# **Model class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKi5ojI9zXl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel:\n",
        "  \n",
        "  def __init__(self, learning_rate=0.01):\n",
        "\n",
        "    self.layers = list()\n",
        "    self.learning_rate = learning_rate\n",
        "    self.history = dict()\n",
        "\n",
        "\n",
        "  def add_layer(self, layer):\n",
        "\n",
        "    if len(self.layers) == 0:\n",
        "\n",
        "      self.layers.append(layer)\n",
        "      self.layers[0]._set_layer_type(\"output_layer\")\n",
        "\n",
        "    else:\n",
        "      \n",
        "      # bind new layer to the current last layer\n",
        "      layer._bind_to(self.layers[-1])\n",
        "\n",
        "      # set new layer as the next layer of current last layer\n",
        "      self.layers[-1]._set_next_layer(layer)\n",
        "\n",
        "      # update new layer to last layer\n",
        "      self.layers.append(layer)\n",
        "      \n",
        "\n",
        "      n = len(self.layers)\n",
        "      last_layer_i = n - 1\n",
        "      before_last_i = n - 2\n",
        "\n",
        "      # set new layer's type as output layer\n",
        "      self.layers[last_layer_i]._set_layer_type(\"output_layer\")\n",
        "\n",
        "      # update old output layer to hidden layer\n",
        "      self.layers[before_last_i]._set_layer_type(\"hidden_layer\")\n",
        "\n",
        "    self.layers[-1]._set_learning_rate(self.learning_rate)\n",
        "\n",
        "\n",
        "  def fit(self, X, Y, epochs, verbose=True):\n",
        "\n",
        "    self.history[\"loss\"] = []\n",
        "    m = len(Y)\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "      if verbose:\n",
        "        print(\"Epoch {}\\n\".format(i+1))\n",
        "\n",
        "      # initialize current output to input\n",
        "      current_input = X\n",
        "\n",
        "      # forward propagation\n",
        "      for j in self.layers:\n",
        "\n",
        "        current_input = j._forward_prop(current_input)\n",
        "\n",
        "      # calculate loss\n",
        "      prediction = current_input.flatten()\n",
        "      logprobs = np.multiply(np.log(prediction), Y) + np.multiply(np.log(1 - prediction), (1 - Y))\n",
        "      cost = - np.sum(logprobs) / m\n",
        "      \n",
        "      # back propagation\n",
        "      da = - (Y / prediction) + (1 - Y)/(1 - prediction)\n",
        "      for j in reversed(range(len(self.layers))):\n",
        "        \n",
        "        da = self.layers[j]._backward_prop(da)\n",
        "      \n",
        "      # save the loss value\n",
        "      self.history[\"loss\"].append(cost)\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "\n",
        "    current_output = X\n",
        "    for i in self.layers:\n",
        "\n",
        "      current_output = i._forward_prop(current_output)\n",
        "    \n",
        "    return current_output\n",
        "\n",
        "  \n",
        "  def print_layers(self,):\n",
        "    print(\"\\n****************************\\n\")\n",
        "\n",
        "    for i, v in enumerate(self.layers):\n",
        "      print(\"Layer Index: {}\\nLayer Type: {}\\nUnits: {}\\nActivation: {}\\n\"\n",
        "      .format(i, v.layer_type, v.units, v.activation, v.input_size))\n",
        "    \n",
        "    print(\"****************************\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfV9ER-VEAHP",
        "colab_type": "text"
      },
      "source": [
        "# **Testing our neural network model with one hidden layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niVW_wLVQ1nj",
        "colab_type": "code",
        "outputId": "a0dde8af-2e50-4327-de76-838de3d0b412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "\n",
        "# X and Y are dummy datas which represent the XOR operation.\n",
        "X = np.array([[1, 1], [1, 0], [0, 1], [0, 0]]).T\n",
        "Y = np.array([0, 1, 1, 0])\n",
        "# get the current time\n",
        "past = time.time()\n",
        "\n",
        "# create the model with learning_rate as a parameter\n",
        "model = MyModel(learning_rate=0.1)\n",
        "\n",
        "# create layers\n",
        "layer_1 = Layer(units=5, input_shape=(2, 1), activation=\"tanh\")\n",
        "layer_2 = Layer(units=1, activation=\"sigmoid\")\n",
        "\n",
        "# add layers to the model\n",
        "model.add_layer(layer_1)\n",
        "model.add_layer(layer_2)\n",
        "\n",
        "# fit the data to the model\n",
        "model.fit(X, Y, 1000, verbose=False)\n",
        "\n",
        "# get prediction after fitting the data\n",
        "# use same data as we used in fitting process\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# calculate the accuracy percentage of the predictions\n",
        "accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
        "\n",
        "print(\"Results after fitting:\\n\", predictions)\n",
        "print(\"\\nAccuracy: {}%\".format(accuracy))\n",
        "print(\"\\nTime passed: {} seconds\".format(time.time() - past))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after fitting:\n",
            " [[0.05368613 0.96696315 0.95600531 0.00767764]]\n",
            "\n",
            "Accuracy: 96.5401173299935%\n",
            "\n",
            "Time passed: 0.09686398506164551 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIPH6QVKREPG",
        "colab_type": "code",
        "outputId": "652e5b71-a6b0-4d25-c688-eb8257543a01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.title(\"Loss vs Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.plot(model.history[\"loss\"])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7b18298400>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MrC_96GCF82",
        "colab_type": "code",
        "outputId": "ef84d1b4-67a5-42d7-e8b6-d02f2fbbc504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# same dummy data as we used before\n",
        "X = np.array([[1, 1], [1, 0], [0, 1], [0, 0]]).T\n",
        "Y = np.array([0, 1, 1, 0])\n",
        "\n",
        "# number of units in a hidden layer\n",
        "unit_numbers = [1, 2, 3, 4, 5, 10, 20, 50, 100]\n",
        "\n",
        "# try each number of hidden units in the for loop and\n",
        "# print the accuracy for each one\n",
        "# epoch is defined as 1000\n",
        "for n_hidden_units in unit_numbers:\n",
        "\n",
        "  model = MyModel(0.1)\n",
        "\n",
        "  layer_1 = Layer(units=n_hidden_units, input_shape=(2, 1), activation=\"tanh\")\n",
        "  layer_2 = Layer(units=1, activation=\"sigmoid\")\n",
        "\n",
        "  model.add_layer(layer_1)\n",
        "  model.add_layer(layer_2)\n",
        "\n",
        "  model.fit(X, Y, epochs=1000, verbose=False)\n",
        "\n",
        "  predictions = model.predict(X)\n",
        "\n",
        "  accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
        "\n",
        "  print(\"Accuracy for {} hidden units: {}%\".format(n_hidden_units, accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for 1 hidden units: 66.16058159748553%\n",
            "Accuracy for 2 hidden units: 97.02922579728171%\n",
            "Accuracy for 3 hidden units: 96.5205345303777%\n",
            "Accuracy for 4 hidden units: 96.63291050169838%\n",
            "Accuracy for 5 hidden units: 96.49812711822749%\n",
            "Accuracy for 10 hidden units: 96.105484108208%\n",
            "Accuracy for 20 hidden units: 96.60991405851951%\n",
            "Accuracy for 50 hidden units: 96.85797035549724%\n",
            "Accuracy for 100 hidden units: 98.68565859730916%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia7vsOBsjWfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}