{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_architecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPARnGDhZnyvjUDtLIWHK2c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halilyaman/neural_network_implementation/blob/master/NN_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1WJ5oHSamXo",
        "colab_type": "text"
      },
      "source": [
        "#### **Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXe8pRunGaiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDKslBsRaxmw",
        "colab_type": "text"
      },
      "source": [
        "# **Activations class**\n",
        "This class contains activation functions and derivatives of them. All functions are static in order to use them directly in a neural network. \\\n",
        "X represents the input matrix. \\\n",
        "\n",
        "---\n",
        " **sigmoid** \\\n",
        "![Sigmoid Function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/400px-Logistic-curve.svg.png)\n",
        "\n",
        "---\n",
        "**tanh** \\\n",
        "![Tanh Function](https://www.medcalc.org/manual/_help/functions/tanh.png)\n",
        "\n",
        "---\n",
        "**ReLU** \\\n",
        "![ReLU Function](https://miro.medium.com/max/400/0*g9ypL5M3k-f7EW85.png)\n",
        "\n",
        "---\n",
        "**Leaky Relu** \\\n",
        "![Leaky ReLU](https://1.bp.blogspot.com/-5ymhxBydo8A/XPj_qXK-sWI/AAAAAAAABU4/UjgZ7eChpwsoPa1_bZjvdrzKCsCfQPaJgCLcBGAs/s400/leaking_relu_2.PNG)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXSoSODaupiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Activations:\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid(X):\n",
        "\n",
        "    return 1 / (1 + np.exp(-X))\n",
        "\n",
        "  @staticmethod\n",
        "  def linear(X):\n",
        "\n",
        "    return X\n",
        "\n",
        "  @staticmethod\n",
        "  def relu(X):\n",
        "\n",
        "    return np.maximum(0, X)\n",
        "\n",
        "  @staticmethod\n",
        "  def leaky_relu(X):\n",
        "\n",
        "    return np.maximum(0.01 * X, X)\n",
        "\n",
        "  @staticmethod\n",
        "  def tanh(X):\n",
        "\n",
        "    return np.tanh(X)\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid_derivative(X):\n",
        "\n",
        "    X_sigmoid = Activations.sigmoid(X)\n",
        "\n",
        "    return X_sigmoid * (1 - X_sigmoid)\n",
        "\n",
        "  @staticmethod\n",
        "  def relu_derivative(X):\n",
        "    \n",
        "    derivatives = np.zeros(X.shape)\n",
        "\n",
        "    for i, rows in enumerate(X):\n",
        "\n",
        "      for j, v in enumerate(rows):\n",
        "\n",
        "        if v < 0:\n",
        "          derivatives[i, j] = 0\n",
        "        else:\n",
        "          derivatives[i, j] = 1\n",
        "\n",
        "    return derivatives\n",
        "\n",
        "  @staticmethod\n",
        "  def leaky_relu_derivative(X):\n",
        "    \n",
        "    derivatives = np.zeros(X.shape)\n",
        "\n",
        "    for i, rows in enumerate(X):\n",
        "\n",
        "      for j, v in enumerate(rows):\n",
        "\n",
        "        if v < 0:\n",
        "          derivatives[i, j] = 0.01\n",
        "        else:\n",
        "          derivatives[i, j] = 1\n",
        "\n",
        "    return derivatives\n",
        "\n",
        "  @staticmethod\n",
        "  def tanh_derivative(X):\n",
        "\n",
        "    tanh = Activations.tanh(X)\n",
        "    \n",
        "    return 1 - tanh ** 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1xfiWQjcLJ_",
        "colab_type": "text"
      },
      "source": [
        "# **Layer class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW01W026GtId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer:\n",
        "\n",
        "  \"\"\"\n",
        "  @Author: Halil Yaman\n",
        "\n",
        "  This class is used for creating a layer for our neural network model.\n",
        "  It is only capable of doing binary classification for now. But others will\n",
        "  be added soon.\n",
        "  \n",
        "  Each new layer comes one after another. You can use the 'add' function in\n",
        "  Model class for adding new layer to another one.\n",
        "\n",
        "  There are two types of layers, hidden layer and output layer.\n",
        "  In the first layer, you must specify the input_shape. For other layers,\n",
        "  you don't need to specify it. It is done automatically.\n",
        "\n",
        "  The number of units in output layer must match with the number of elements \n",
        "  in your one output sample.\n",
        "\n",
        "  All calculations for back and forward propagations are vectorized by using\n",
        "  numpy library. So, efficiency is improved by utilizing the multiple cores.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, units, input_shape=None, activation=\"linear\"):\n",
        "\n",
        "    \"\"\"\n",
        "    :param units: Dimension of the output space\n",
        "    :param input_shape: Shape of the one single input\n",
        "    :param activation: Activation function used in this specific layer.\n",
        "    \"\"\"\n",
        "\n",
        "    self.units = units\n",
        "    self.input_shape = input_shape\n",
        "    self.activation = activation\n",
        "    self.inputs = None\n",
        "    self.learning_rate = None\n",
        "    self.activation_derivative = None\n",
        "    self.z = None\n",
        "\n",
        "    if not(self.input_shape == None):\n",
        "      self.__init_weights()\n",
        "  \n",
        "  def __init_weights(self,):\n",
        "\n",
        "    self.w = np.random.rand(self.units, self.input_shape[0])\n",
        "    self.b = np.zeros((self.units, 1))\n",
        "\n",
        "  def _forward_prop(self, X):\n",
        "\n",
        "    if X.shape[0] != self.input_shape[0]:\n",
        "      raise Exception(\"input shape doesn't match with the data!\")\n",
        "\n",
        "    self.inputs = X\n",
        "    dot_products = np.dot(self.w, X)\n",
        "\n",
        "    # cache 'z' for back propagation\n",
        "    self.z = dot_products + self.b\n",
        "\n",
        "    output = self._choose_activation(self.z)\n",
        "\n",
        "    return output\n",
        "  \n",
        "  def _backward_prop(self, da):\n",
        "\n",
        "    self.d_z = da * self.activation_derivative(self.z)\n",
        "    \n",
        "    # calculate new weight and bias values\n",
        "    avg_factor = (1 / len(self.inputs))\n",
        "    d_w = avg_factor * self.d_z.dot(self.inputs.T)\n",
        "    d_b = avg_factor * np.sum(self.d_z, axis=1, keepdims=True)\n",
        "\n",
        "    # updating weights and b\n",
        "    self.b = self.b - self.learning_rate * d_b\n",
        "    self.w = self.w - self.learning_rate * d_w\n",
        "\n",
        "    # calculate 'da' and return it to the previous layer\n",
        "    da = self.w.T.dot(self.d_z)\n",
        "\n",
        "    return da\n",
        "\n",
        "\n",
        "  def _choose_activation(self, X):\n",
        "\n",
        "    if self.activation == \"sigmoid\":\n",
        "      X = Activations.sigmoid(X)\n",
        "      self.activation_derivative = Activations.sigmoid_derivative\n",
        "    \n",
        "    elif self.activation == \"linear\":\n",
        "      X = Activations.linear(X)\n",
        "    \n",
        "    elif self.activation == \"relu\":\n",
        "      X = Activations.relu(X)\n",
        "      self.activation_derivative = Activations.relu_derivative\n",
        "\n",
        "    elif self.activation == \"leaky_relu\":\n",
        "      X = Activations.leaky_relu(X)\n",
        "      self.activation_derivative = Activations.leaky_relu_derivative\n",
        "\n",
        "    elif self.activation == \"tanh\":\n",
        "      X = Activations.tanh(X)\n",
        "      self.activation_derivative = Activations.tanh_derivative\n",
        "\n",
        "    return X\n",
        "    \n",
        "  def _bind_to(self, layer):\n",
        "\n",
        "    self.input_shape = (layer.units, 1)\n",
        "    self.__init_weights()\n",
        "\n",
        "  def _set_learning_rate(self, learning_rate):\n",
        "    \n",
        "    self.learning_rate = learning_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rD9tp5FcdwM",
        "colab_type": "text"
      },
      "source": [
        "# **Model class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKi5ojI9zXl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel:\n",
        "  \n",
        "  def __init__(self, learning_rate=0.01):\n",
        "\n",
        "    self.layers = list()\n",
        "    self.learning_rate = learning_rate\n",
        "    self.history = dict()\n",
        "\n",
        "\n",
        "  def add_layer(self, layer):\n",
        "\n",
        "    if len(self.layers) == 0:\n",
        "\n",
        "      self.layers.append(layer)\n",
        "\n",
        "    else:\n",
        "      \n",
        "      # bind new layer to the current last layer\n",
        "      layer._bind_to(self.layers[-1])\n",
        "\n",
        "      # add new layer to the end of layers\n",
        "      self.layers.append(layer)\n",
        "\n",
        "    self.layers[-1]._set_learning_rate(self.learning_rate)\n",
        "\n",
        "\n",
        "  def fit(self, X, Y, epochs, verbose=True):\n",
        "\n",
        "    self.history[\"loss\"] = []\n",
        "    m = len(Y)\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "      if verbose:\n",
        "        print(\"Epoch {}\\n\".format(i+1))\n",
        "\n",
        "      # initialize current input\n",
        "      current_input = X\n",
        "\n",
        "      # forward propagation\n",
        "      for j in self.layers:\n",
        "        \n",
        "        # update current input to put into the next layer\n",
        "        current_input = j._forward_prop(current_input)\n",
        "\n",
        "      # calculate loss\n",
        "      prediction = current_input.flatten()\n",
        "      logprobs = np.multiply(np.log(prediction), Y) + np.multiply(np.log(1 - prediction), (1 - Y))\n",
        "      cost = - np.sum(logprobs) / m\n",
        "\n",
        "      # save the loss value\n",
        "      self.history[\"loss\"].append(cost)\n",
        "      \n",
        "      # back propagation\n",
        "      da = - (Y / prediction) + (1 - Y)/(1 - prediction)\n",
        "      for j in reversed(range(len(self.layers))):\n",
        "        \n",
        "        da = self.layers[j]._backward_prop(da)\n",
        "      \n",
        "\n",
        "  def predict(self, X):\n",
        "\n",
        "    current_output = X\n",
        "    for i in self.layers:\n",
        "\n",
        "      current_output = i._forward_prop(current_output)\n",
        "    \n",
        "    return current_output\n",
        "\n",
        "  \n",
        "  def print_layers(self,):\n",
        "    print(\"\\n****************************\\n\")\n",
        "\n",
        "    for i, v in enumerate(self.layers):\n",
        "      print(\"Layer Index: {}\\nUnits: {}\\nActivation: {}\\n\"\n",
        "      .format(i, v.layer_type, v.units, v.activation, v.input_size))\n",
        "    \n",
        "    print(\"****************************\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfV9ER-VEAHP",
        "colab_type": "text"
      },
      "source": [
        "# **Testing our neural network model with one hidden layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niVW_wLVQ1nj",
        "colab_type": "code",
        "outputId": "94942447-ec05-4221-f2a3-a120fb54c769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "\n",
        "# X and Y are dummy datas which represent the XOR operation.\n",
        "X = np.array([[1, 1], [1, 0], [0, 1], [0, 0]]).T\n",
        "Y = np.array([0, 1, 1, 0])\n",
        "\n",
        "# get the current time\n",
        "past = time.time()\n",
        "\n",
        "# create the model with learning_rate as a parameter\n",
        "model = MyModel(learning_rate=0.1)\n",
        "\n",
        "# create layers\n",
        "layer_1 = Layer(units=5, input_shape=(2, 1), activation=\"tanh\")\n",
        "layer_2 = Layer(units=1, activation=\"sigmoid\")\n",
        "\n",
        "# add layers to the model\n",
        "model.add_layer(layer_1)\n",
        "model.add_layer(layer_2)\n",
        "\n",
        "# fit the data to the model\n",
        "model.fit(X, Y, 1000, verbose=False)\n",
        "\n",
        "# get prediction after fitting the data\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# calculate the accuracy percentage of the predictions\n",
        "accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
        "\n",
        "print(\"Predictions:\\n{}\".format(predictions))\n",
        "print(\"\\nAccuracy: {}%\".format(accuracy))\n",
        "print(\"\\nTime passed: {} seconds\".format(time.time() - past))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predictions:\n",
            "[[0.05503172 0.96644369 0.96326807 0.01361381]]\n",
            "\n",
            "Accuracy: 96.52665570621737%\n",
            "\n",
            "Time passed: 0.0966484546661377 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIPH6QVKREPG",
        "colab_type": "code",
        "outputId": "c50bfbe8-770d-45d3-9beb-daad4f37d183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.title(\"Loss vs Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.plot(model.history[\"loss\"])"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7b17b1b7f0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MrC_96GCF82",
        "colab_type": "code",
        "outputId": "9b92fed8-226f-4966-c0d9-c81f731c9d2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# same dummy data as we used before\n",
        "X = np.array([[1, 1], [1, 0], [0, 1], [0, 0]]).T\n",
        "Y = np.array([0, 1, 1, 0])\n",
        "\n",
        "# number of units in a hidden layer\n",
        "unit_numbers = [1, 2, 3, 4, 5, 10, 20, 50, 100]\n",
        "\n",
        "# try each number of hidden units in the for loop and\n",
        "# print the accuracy for each one\n",
        "# epoch is defined as 1000\n",
        "for n_hidden_units in unit_numbers:\n",
        "\n",
        "  model = MyModel(0.1)\n",
        "\n",
        "  layer_1 = Layer(units=n_hidden_units, input_shape=(2, 1), activation=\"tanh\")\n",
        "  layer_2 = Layer(units=1, activation=\"sigmoid\")\n",
        "\n",
        "  model.add_layer(layer_1)\n",
        "  model.add_layer(layer_2)\n",
        "\n",
        "  model.fit(X, Y, epochs=1000, verbose=False)\n",
        "\n",
        "  predictions = model.predict(X)\n",
        "\n",
        "  accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
        "\n",
        "  print(\"Accuracy for {} hidden units: {}%\".format(n_hidden_units, accuracy))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for 1 hidden units: 66.11244074420398%\n",
            "Accuracy for 2 hidden units: 97.31841337041922%\n",
            "Accuracy for 3 hidden units: 73.33139670411832%\n",
            "Accuracy for 4 hidden units: 96.26644800757379%\n",
            "Accuracy for 5 hidden units: 94.73269913440706%\n",
            "Accuracy for 10 hidden units: 95.32176612142365%\n",
            "Accuracy for 20 hidden units: 95.54826879974321%\n",
            "Accuracy for 50 hidden units: 97.41262298476092%\n",
            "Accuracy for 100 hidden units: 98.30665825914598%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia7vsOBsjWfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}