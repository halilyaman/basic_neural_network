{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_architecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/wlVGtpA6EI1IZ4PUnrrk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halilyaman/neural_network_implementation/blob/master/NN_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1WJ5oHSamXo",
        "colab_type": "text"
      },
      "source": [
        "#### **Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXe8pRunGaiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDKslBsRaxmw",
        "colab_type": "text"
      },
      "source": [
        "# **Activations class**\n",
        "This class contains activation functions and derivatives of them. All functions are static in order to use them directly in a neural network. \\\n",
        "X represents the input matrix. \\\n",
        "\n",
        "---\n",
        " **sigmoid** \\\n",
        "![Sigmoid Function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/400px-Logistic-curve.svg.png)\n",
        "\n",
        "---\n",
        "**tanh** \\\n",
        "![Tanh Function](https://www.medcalc.org/manual/_help/functions/tanh.png)\n",
        "\n",
        "---\n",
        "**ReLU** \\\n",
        "![ReLU Function](https://miro.medium.com/max/400/0*g9ypL5M3k-f7EW85.png)\n",
        "\n",
        "---\n",
        "**Leaky Relu** \\\n",
        "![Leaky ReLU](https://1.bp.blogspot.com/-5ymhxBydo8A/XPj_qXK-sWI/AAAAAAAABU4/UjgZ7eChpwsoPa1_bZjvdrzKCsCfQPaJgCLcBGAs/s400/leaking_relu_2.PNG)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXSoSODaupiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Activations:\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid(X):\n",
        "\n",
        "    return 1 / (1 + np.exp(-X))\n",
        "\n",
        "  @staticmethod\n",
        "  def linear(X):\n",
        "\n",
        "    return X\n",
        "\n",
        "  @staticmethod\n",
        "  def relu(X):\n",
        "\n",
        "    return np.maximum(0, X)\n",
        "\n",
        "  @staticmethod\n",
        "  def leaky_relu(X):\n",
        "\n",
        "    return np.maximum(0.01 * X, X)\n",
        "\n",
        "  @staticmethod\n",
        "  def tanh(X):\n",
        "\n",
        "    return np.tanh(X)\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid_derivative(X):\n",
        "\n",
        "    X_sigmoid = Activations.sigmoid(X)\n",
        "\n",
        "    return X_sigmoid * (1 - X_sigmoid)\n",
        "\n",
        "  @staticmethod\n",
        "  def relu_derivative(X):\n",
        "    \n",
        "    derivatives = np.zeros(X.shape)\n",
        "\n",
        "    for i, rows in enumerate(X):\n",
        "\n",
        "      for j, v in enumerate(rows):\n",
        "\n",
        "        if v < 0:\n",
        "          derivatives[i, j] = 0\n",
        "        else:\n",
        "          derivatives[i, j] = 1\n",
        "\n",
        "    return derivatives\n",
        "\n",
        "  @staticmethod\n",
        "  def leaky_relu_derivative(X):\n",
        "    \n",
        "    derivatives = np.zeros(X.shape)\n",
        "\n",
        "    for i, rows in enumerate(X):\n",
        "\n",
        "      for j, v in enumerate(rows):\n",
        "\n",
        "        if v < 0:\n",
        "          derivatives[i, j] = 0.01\n",
        "        else:\n",
        "          derivatives[i, j] = 1\n",
        "\n",
        "    return derivatives\n",
        "\n",
        "  @staticmethod\n",
        "  def tanh_derivative(X):\n",
        "\n",
        "    tanh = Activations.tanh(X)\n",
        "    \n",
        "    return 1 - tanh ** 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1xfiWQjcLJ_",
        "colab_type": "text"
      },
      "source": [
        "# **Layer class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW01W026GtId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer:\n",
        "\n",
        "  \"\"\"\n",
        "  @Author: Halil Yaman\n",
        "\n",
        "  This class is used for creating a layer for our neural network model.\n",
        "  It is only capable of doing binary classification for now. But others will\n",
        "  be added soon.\n",
        "  \n",
        "  Each new layer comes one after another. You can use the 'add' function in\n",
        "  Model class for adding new layer to another one.\n",
        "\n",
        "  There are two types of layers, hidden layer and output layer.\n",
        "  In the first layer, you must specify the input_shape. For other layers,\n",
        "  you don't need to specify it. It is done automatically.\n",
        "\n",
        "  The number of units in output layer must match with the number of elements \n",
        "  in your one output sample.\n",
        "\n",
        "  All calculations for back and forward propagations are vectorized by using\n",
        "  numpy library. So, efficiency is improved by utilizing the multiple cores.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, units, input_shape=None, activation=\"linear\"):\n",
        "\n",
        "    \"\"\"\n",
        "    :param units: Dimension of the output space\n",
        "    :param input_shape: Shape of the one single input\n",
        "    :param activation: Activation function used in this specific layer.\n",
        "    \"\"\"\n",
        "\n",
        "    self.units = units\n",
        "    self.input_shape = input_shape\n",
        "    self.activation = activation\n",
        "    self.layer_type = None\n",
        "    self.predictions = None\n",
        "    self.inputs = None\n",
        "    self.learning_rate = None\n",
        "    self.next_layer = None\n",
        "    self.activation_derivative = None\n",
        "    self.z = None\n",
        "\n",
        "    if not(self.input_shape == None):\n",
        "      self.__init_weights()\n",
        "  \n",
        "  def __init_weights(self,):\n",
        "\n",
        "    self.w = np.random.rand(self.units, self.input_shape[0])\n",
        "    self.b = np.zeros((self.units, 1))\n",
        "\n",
        "  def _forward_prop(self, X):\n",
        "\n",
        "    if X.shape[0] != self.input_shape[0]:\n",
        "      raise Exception(\"input shape doesn't match with the data!\")\n",
        "\n",
        "    self.inputs = X\n",
        "\n",
        "    dot_products = np.dot(self.w, X)\n",
        "    pred = dot_products + self.b\n",
        "    self.z = pred\n",
        "    self.predictions = self._choose_activation(pred)\n",
        "\n",
        "    return self.predictions\n",
        "  \n",
        "  def _backward_prop(self, Y):\n",
        "\n",
        "    avg_factor = (1 / len(self.inputs))\n",
        "\n",
        "    # derivative calculations are different in order to layer type.\n",
        "    # So we need these conditions\n",
        "    if self.layer_type == \"output_layer\":\n",
        "\n",
        "      # calculating new weights and bias\n",
        "      self.d_z = self.predictions - Y\n",
        "    \n",
        "    if self.layer_type == \"hidden_layer\":\n",
        "\n",
        "      # calculating new weights and bias\n",
        "      self.d_z = self.next_layer.w.T.dot(self.next_layer.d_z) * self.activation_derivative(self.z)\n",
        "    \n",
        "    # calculate new weight and bias values\n",
        "    d_w = avg_factor * self.d_z.dot(self.inputs.T)\n",
        "    d_b = avg_factor * np.sum(self.d_z, axis=1, keepdims=True)\n",
        "\n",
        "    # updating weights and b\n",
        "    self.b = self.b - self.learning_rate * d_b\n",
        "    self.w = self.w - self.learning_rate * d_w\n",
        "\n",
        "\n",
        "  def _choose_activation(self, X):\n",
        "\n",
        "    if self.activation == \"sigmoid\":\n",
        "      X = Activations.sigmoid(X)\n",
        "      self.activation_derivative = Activations.sigmoid_derivative\n",
        "    \n",
        "    elif self.activation == \"linear\":\n",
        "      X = Activations.linear(X)\n",
        "    \n",
        "    elif self.activation == \"relu\":\n",
        "      X = Activations.relu(X)\n",
        "      self.activation_derivative = Activations.relu_derivative\n",
        "\n",
        "    elif self.activation == \"leaky_relu\":\n",
        "      X = Activations.leaky_relu(X)\n",
        "      self.activation_derivative = Activations.leaky_relu_derivative\n",
        "\n",
        "    elif self.activation == \"tanh\":\n",
        "      X = Activations.tanh(X)\n",
        "      self.activation_derivative = Activations.tanh_derivative\n",
        "\n",
        "    return X\n",
        "    \n",
        "  def _bind_to(self, layer):\n",
        "\n",
        "    self.input_shape = (layer.units, 1)\n",
        "    self.__init_weights()\n",
        "\n",
        "  def _set_layer_type(self, layer_type):\n",
        "\n",
        "    self.layer_type = layer_type\n",
        "\n",
        "  def _set_learning_rate(self, learning_rate):\n",
        "    \n",
        "    self.learning_rate = learning_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rD9tp5FcdwM",
        "colab_type": "text"
      },
      "source": [
        "# **Model class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKi5ojI9zXl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel:\n",
        "  \n",
        "  def __init__(self, learning_rate=0.01):\n",
        "\n",
        "    self.layers = list()\n",
        "    self.learning_rate = learning_rate\n",
        "    self.history = dict()\n",
        "\n",
        "\n",
        "  def add_layer(self, layer):\n",
        "\n",
        "    if len(self.layers) == 0:\n",
        "\n",
        "      self.layers.append(layer)\n",
        "      self.layers[0]._set_layer_type(\"output_layer\")\n",
        "\n",
        "    else:\n",
        "\n",
        "      layer._bind_to(self.layers[-1])\n",
        "      self.layers[-1].next_layer = layer\n",
        "      self.layers.append(layer)\n",
        "      \n",
        "\n",
        "      n = len(self.layers)\n",
        "      last_layer_i = n - 1\n",
        "      before_last_i = n - 2\n",
        "\n",
        "      self.layers[last_layer_i]._set_layer_type(\"output_layer\")\n",
        "      self.layers[before_last_i]._set_layer_type(\"hidden_layer\")\n",
        "\n",
        "    self.layers[-1]._set_learning_rate(self.learning_rate)\n",
        "\n",
        "\n",
        "  def fit(self, X, Y, epochs, verbose=True):\n",
        "\n",
        "    self.history[\"loss\"] = []\n",
        "    m = len(Y)\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "      if verbose:\n",
        "        print(\"Epoch {}\\n\".format(i+1))\n",
        "  \n",
        "      current_output = X\n",
        "      for j in self.layers:\n",
        "\n",
        "        current_output = j._forward_prop(current_output)\n",
        "      \n",
        "      for j in reversed(range(len(self.layers))):\n",
        "\n",
        "        self.layers[j]._backward_prop(Y)\n",
        "      \n",
        "      logprobs = np.multiply(np.log(current_output.flatten()), Y) + np.multiply(np.log(1-current_output.flatten()), (1-Y))\n",
        "      cost = - np.sum(logprobs) / m\n",
        "      \n",
        "      self.history[\"loss\"].append(cost)\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "\n",
        "    current_output = X\n",
        "    for i in self.layers:\n",
        "\n",
        "      current_output = i._forward_prop(current_output)\n",
        "    \n",
        "    return current_output\n",
        "\n",
        "  \n",
        "  def print_layers(self,):\n",
        "    print(\"\\n****************************\\n\")\n",
        "\n",
        "    for i, v in enumerate(self.layers):\n",
        "      print(\"Layer Index: {}\\nLayer Type: {}\\nUnits: {}\\nActivation: {}\\n\"\n",
        "      .format(i, v.layer_type, v.units, v.activation, v.input_size))\n",
        "    \n",
        "    print(\"****************************\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfV9ER-VEAHP",
        "colab_type": "text"
      },
      "source": [
        "# **Testing our neural network model with one hidden layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niVW_wLVQ1nj",
        "colab_type": "code",
        "outputId": "89aae96e-dbae-4ae9-ca75-c8f6b5d0c907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "\n",
        "# X and Y are dummy datas which represent the XOR operation.\n",
        "X = np.array([[1, 1], [1, 0], [0, 1], [0, 0]]).T\n",
        "Y = np.array([0, 1, 1, 0])\n",
        "# get the current time\n",
        "past = time.time()\n",
        "\n",
        "# create the model with learning_rate as a parameter\n",
        "model = MyModel(learning_rate=0.1)\n",
        "\n",
        "# create layers\n",
        "layer_1 = Layer(units=2, input_shape=(2, 1), activation=\"tanh\")\n",
        "layer_2 = Layer(units=1, activation=\"sigmoid\")\n",
        "\n",
        "# add layers to the model\n",
        "model.add_layer(layer_1)\n",
        "model.add_layer(layer_2)\n",
        "\n",
        "# fit the data to the model\n",
        "model.fit(X, Y, 20000, verbose=False)\n",
        "\n",
        "# get prediction after fitting the data\n",
        "# use same data as we used in fitting process\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# calculate the accuracy percentage of the predictions\n",
        "accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
        "\n",
        "print(\"Results after fitting:\\n\", predictions)\n",
        "print(\"\\nAccuracy: {}%\".format(accuracy))\n",
        "print(\"\\nTime passed: {} seconds\".format(time.time() - past))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after fitting:\n",
            " [[0.00105106 0.99946256 0.99946231 0.0010164 ]]\n",
            "\n",
            "Accuracy: 99.92143538036488%\n",
            "\n",
            "Time passed: 1.4722137451171875 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIPH6QVKREPG",
        "colab_type": "code",
        "outputId": "7977437f-9ea1-4a40-b2eb-affc21bbaf3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.title(\"Loss vs Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.plot(model.history[\"loss\"])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7b18482208>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAdTklEQVR4nO3de5ScdZ3n8fenqrtDEkhMTC8HkpAEuTsqcbIoo+iMCgZHwctZDYuKjrOsOjDed3B1WYbx7HpHcZhxcGS8QkRWjxnNDKB4myNIAkYgYEgTwSRcEggh5EL69t0/nl91nqp+uunu9FPVdH9e59RJPb96qurbT3fqU7/f77koIjAzM2tUaXUBZmY2MTkgzMyskAPCzMwKOSDMzKyQA8LMzAo5IMzMrJADwmyKkfQOSf/R6jps4nNA2DOepPslvarVdYyFpD+V1C9pd8Pt1FbXZtbW6gLMjAcjYkGrizBr5B6ETVqSpkn6gqQH0+0Lkqalx+ZJ+qGknZJ2SPqlpEp67G8kbZX0pKQNkl5Z8NovkvSwpGqu7Q2S7kj3T5G0VtIuSY9I+vwYf4afSfq/km5Nr/UDSXNzj58laX36OX4m6cTcYwslfU/SdkmPSfr7htf+rKTHJf1e0pljqc8mNweETWYfA14MnAy8ADgF+Hh67EPAFqATOBz4n0BIOh64APjPEXEY8Grg/sYXjohfA3uAV+Sa/ytwdbr/ReCLETELeA5w7UH8HG8H/gI4AugFLgeQdBxwDfD+9HOsBv5VUkcKrh8CDwCLgfnAytxrvgjYAMwDPg18VZIOokabhBwQNpmdC1waEdsiYjvwt8Db0mM9ZB+4iyKiJyJ+GdmJyfqAacBJktoj4v6IuG+I178GOAdA0mHAa1Jb7fWPkTQvInZHxC3D1Hlk6gHkbzNzj38zIu6KiD3A/wLenALgLcCPIuLGiOgBPgtMB/6ELAyPBD4SEXsi4qmIyE9MPxARX4mIPuDraVscPuzWtCnHAWGT2ZFk36BrHkhtAJ8BuoAbJG2SdBFARHSRfSO/BNgmaaWkIyl2NfDGNGz1RuD2iKi937uA44DfSVoj6bXD1PlgRDyr4bYn9/jmhp+hneybf93PFxH9ad35wEKyEOgd4j0fzj1vb7p76DA12hTkgLDJ7EFgUW75qNRGRDwZER+KiKOBs4AP1uYaIuLqiHhpem4Anyp68Yi4m+wD+kzqh5eIiI0RcQ7wn9Lzr2voFYzGwoafoQd4tPHnS0NEC4GtZEFxlCTviGJj5oCwyaJd0iG5WxvZcM/HJXVKmgdcDHwLQNJrJR2TPlSfIBta6pd0vKRXpF7BU8A+oH+Y970aeB/wMuC7tUZJb5XUmb7V70zNw73OcN4q6SRJM4BLgevS0NC1wJ9LeqWkdrJ5lf3Ar4BbgYeAT0qambbJS8b4/jZFOSBsslhN9mFeu10CfAJYC9wB3AncntoAjgV+DOwGbgb+ISJ+Sjb/8Emyb+gPk/UAPjrM+14DvBy4KSIezbUvB9ZL2k02Yb0iIvYN8RpHFhwH8abc498EvpbqOQT4a4CI2AC8FfhSqvd1wOsiojsFyOuAY4A/kE3Iv2WYn8NsEPmCQWYTl6SfAd+KiH9udS029bgHYWZmhRwQZmZWyENMZmZWyD0IMzMrNGn2kZ43b14sXry41WWYmT2j3HbbbY9GRGfRY5MmIBYvXszatWtbXYaZ2TOKpAeGesxDTGZmVsgBYWZmhRwQZmZWqNSAkLQ8XXClq3a2zIbHL5O0Lt3ulbSz4fFZkrY0XujEzMzKV9okdTpf/RXA6WTngVkjaVU6AyYAEfGB3PoXAksbXubvgF+UVaOZmQ2tzB7EKUBXRGyKiG6yq1mdPcz653DgYitI+mOyC5jcUGKNZmY2hDIDYj71FzrZktoGkbQIWALclJYrwOeADw/3BpLOT9f9Xbt9+/ZxKdrMzDITZZJ6BQfOcQ/wXmB1RGwZ7kkRcWVELIuIZZ2dhcd5PK09+3v5/A0b+M0fHh/T883MJqsyD5TbSv2VsBaktiIrgL/KLZ8KnCbpvWSXQeyQtDsiBk10H6ynevq4/KYu5h02jaVHzRnvlzcze8YqMyDWAMdKWkIWDCvILstYR9IJwByyi7YAEBHn5h5/B7CsjHAAqEgA9Pf7pIVmZnmlDTGli6VfAFwP3ANcGxHrJV0q6azcqiuAldGi08oOBITzwcysTqnnYoqI1WSXgsy3XdywfMnTvMbXyC63WAqliOz3ac/NzOpMlEnqlqn1IJwPZmb1HBBZPrgHYWbWYMoHhPAchJlZEQdE6kEETggzs7wpHxCegzAzK+aAqM1BeIzJzKyOA8LHQZiZFZryASHvxWRmVsgBMTAH4YAwM8ub8gEB2TyE48HMrJ4DgmwewkNMZmb1HBDUAqLVVZiZTSwOCLKJavcgzMzqOSDIehDOBzOzeg4IsklqHyhnZlbPAUG2q6vzwcysngOCbA7CJ+szM6vngMBzEGZmRRwQpDkIJ4SZWR0HBD5QzsysiAMCT1KbmRVxQJAmqd2DMDOr44AgnazP+WBmVscBgecgzMyKOCDwyfrMzIo4IPDJ+szMijgg8IFyZmZFHBD4QDkzsyIOCHwchJlZEQcEPg7CzKyIAwLPQZiZFXFA4DkIM7MiDgh8oJyZWZFSA0LSckkbJHVJuqjg8cskrUu3eyXtTO2LJN2e2tdLenfJdXqS2sysQVtZLyypClwBnA5sAdZIWhURd9fWiYgP5Na/EFiaFh8CTo2I/ZIOBe5Kz32wlFrxJLWZWaMyexCnAF0RsSkiuoGVwNnDrH8OcA1ARHRHxP7UPq3kOqlUfLI+M7NGZX7wzgc255a3pLZBJC0ClgA35doWSrojvcanyuo9gOcgzMyKTJRJ6hXAdRHRV2uIiM0R8XzgGOA8SYc3PknS+ZLWSlq7ffv2Mb+55yDMzAYrMyC2AgtzywtSW5EVpOGlRqnncBdwWsFjV0bEsohY1tnZOeZCvZurmdlgZQbEGuBYSUskdZCFwKrGlSSdAMwBbs61LZA0Pd2fA7wU2FBWoT5QzsxssNL2YoqIXkkXANcDVeCqiFgv6VJgbUTUwmIFsDLqdyM6EficpCDbyeizEXFnWbW6B2FmNlhpAQEQEauB1Q1tFzcsX1LwvBuB55dZW57wJLWZWaOJMkndUvI1qc3MBnFA4DkIM7MiDgiyA+U8xGRmVs8BgQ+UMzMr4oDAB8qZmRVxQOCT9ZmZFXFAUDsOotVVmJlNLA4I0l5MOCHMzPIcEKQ5iP5WV2FmNrE4IPCpNszMijgg8IFyZmZFHBD4QDkzsyIOCHyyPjOzIg4I0sn6Wl2EmdkE44DAcxBmZkUcEHgvJjOzIg4IfLI+M7MiDgh8oJyZWREHBB5iMjMr4oAAqhXR57P1mZnVcUAAlYrnIMzMGjkggKrcgzAza+SAwENMZmZFHBDUdnNtdRVmZhOLAwKo+mR9ZmaDOCDIehAeYjIzq+eAwHsxmZkVcUDgvZjMzIo4IKj1ICDcizAzG+CAIOtBAN6TycwsxwFBthcT4GEmM7McBwTZEBN4V1czszwHBNluruCAMDPLc0BwYA7CQ0xmZgeUGhCSlkvaIKlL0kUFj18maV263StpZ2o/WdLNktZLukPSW8qsc2CIyRcNMjMb0FbWC0uqAlcApwNbgDWSVkXE3bV1IuIDufUvBJamxb3A2yNio6QjgdskXR8RO8uotZrlA30eYjIzG1BmD+IUoCsiNkVEN7ASOHuY9c8BrgGIiHsjYmO6/yCwDegsq9BqxUNMZmaNygyI+cDm3PKW1DaIpEXAEuCmgsdOATqA+woeO1/SWklrt2/fPuZCvReTmdlgE2WSegVwXUT05RslHQF8E3hnRAyaIYiIKyNiWUQs6+wcewfDk9RmZoOVGRBbgYW55QWprcgK0vBSjaRZwI+Aj0XELaVUmHg3VzOzwcoMiDXAsZKWSOogC4FVjStJOgGYA9yca+sAvg98IyKuK7FGwHsxmZkVKS0gIqIXuAC4HrgHuDYi1ku6VNJZuVVXACuj/kx5bwZeBrwjtxvsyWXVOnCqDfcgzMwGlLabK0BErAZWN7Rd3LB8ScHzvgV8q8za8iqegzAzG2SiTFK3VNV7MZmZDeKAwHsxmZkVGVFASJopqZLuHyfpLEnt5ZbWPBUfKGdmNshIexC/AA6RNB+4AXgb8LWyimq2Wg/CI0xmZgeMNCAUEXuBNwL/EBH/BXhueWU1V8V7MZmZDTLigJB0KnAu2cFrANVySmo+78VkZjbYSAPi/cBHge+nYxmOBn5aXlnN5b2YzMwGG9FxEBHxc+DnAGmy+tGI+OsyC2sm78VkZjbYSPdiulrSLEkzgbuAuyV9pNzSmufAqTYcEGZmNSMdYjopInYBrwf+jezU3G8rraomG7gehIeYzMwGjDQg2tNxD68HVkVEDzBpPk09SW1mNthIA+KfgPuBmcAv0gV+dpVVVLOlDoSPgzAzyxnpJPXlwOW5pgck/Vk5JTWfLzlqZjbYSCepZ0v6fO3ynpI+R9abmBQGhpjchTAzGzDSIaargCfJrtPwZrLhpX8pq6hmq3ovJjOzQUZ6PYjnRMSbcst/K2ldGQW1QlsKiF4HhJnZgJH2IPZJemltQdJLgH3llNR8noMwMxtspD2IdwPfkDQ7LT8OnFdOSc3Xnq456h6EmdkBI92L6bfACyTNSsu7JL0fuKPM4pql1oPo7etvcSVmZhPHqK4oFxG70hHVAB8soZ6WaKt6DsLMrNHBXHJU41ZFi7WlC0K4B2FmdsDBBMSk+bpd9V5MZmaDDDsHIelJioNAwPRSKmqB9qr3YjIzazRsQETEYc0qpJXcgzAzG+xghpgmjfaBOQgHhJlZjQOC7IJBEvT2e5LazKzGAZG0VyoeYjIzy3FAJNWKvJurmVmOAyJpq8o9CDOzHAdE0laRJ6nNzHIcEElb1XMQZmZ5DoikzXMQZmZ1HBBJW1U+ktrMLMcBkbRVKvQ4IMzMBpQaEJKWS9ogqUvSRQWPXyZpXbrdK2ln7rF/l7RT0g/LrLGmWhF9PlDOzGzASK8oN2qSqsAVwOnAFmCNpFURcXdtnYj4QG79C4GluZf4DDAD+O9l1ZjXVhE93ovJzGxAmT2IU4CuiNgUEd3ASuDsYdY/B7imthARPwGeLLG+Op6DMDOrV2ZAzAc255a3pLZBJC0ClgA3jeYNJJ0vaa2ktdu3bx9zoZDNQXg3VzOzAybKJPUK4LqI6BvNkyLiyohYFhHLOjs7D6oA7+ZqZlavzIDYCizMLS9IbUVWkBteagWfasPMrF6ZAbEGOFbSEkkdZCGwqnElSScAc4CbS6zlabVVKu5BmJnllBYQEdELXABcD9wDXBsR6yVdKums3KorgJURUff1XdIvge8Cr5S0RdKry6oV3IMwM2tU2m6uABGxGljd0HZxw/IlQzz3tPIqG6yjWqG71z0IM7OaiTJJ3XIdbQ4IM7M8B0TS0VZhvwPCzGyAAyKZ1lah25PUZmYDHBCJ5yDMzOo5IJJ2B4SZWR0HRNLhISYzszoOiKSjrUJff/iEfWZmiQMi6WjLNoWHmczMMg6IpKPqgDAzy3NAJNNqPQjPQ5iZAQ6IAR0OCDOzOg6IxHMQZmb1HBBJR7UKOCDMzGocEIl7EGZm9RwQyYE5iFFd9dTMbNJyQCS1vZie6nEPwswMHBADZnZk107a2+0ehJkZOCAGTO/IJqn3dve2uBIzs4nBAZHMnFYLCPcgzMzAATFgRruHmMzM8hwQycAQ034PMZmZgQNiQEdbhfaq2NvjHoSZGTgg6kxvr7LPQ0xmZoADos7MaW3s8RCTmRnggKgzvaPqISYzs8QBkTOzo82T1GZmiQMiZ3pH1bu5mpklDoicmR1V9nmIycwMcEDUmdHRxm4PMZmZAQ6IOrOmt7NrnwPCzAwcEHXmzmzn8b3dRESrSzEzazkHRM6cGR309Qe7nnIvwszMAZEzd2YHAI/v6W5xJWZmrVdqQEhaLmmDpC5JFxU8fpmkdel2r6SducfOk7Qx3c4rs86aOSkgdux1QJiZtZX1wpKqwBXA6cAWYI2kVRFxd22diPhAbv0LgaXp/lzgfwPLgABuS899vKx6AebOcA/CzKymzB7EKUBXRGyKiG5gJXD2MOufA1yT7r8auDEidqRQuBFYXmKtwIEhph0OCDOzUgNiPrA5t7wltQ0iaRGwBLhptM8dT7UhpsccEGZmE2aSegVwXUSM6jBmSedLWitp7fbt2w+6iEOntXHYIW08tHPfQb+WmdkzXZkBsRVYmFtekNqKrODA8NKInxsRV0bEsohY1tnZeZDlZhbOmcHmxx0QZmZlBsQa4FhJSyR1kIXAqsaVJJ0AzAFuzjVfD5whaY6kOcAZqa10C+ZMZ/OOvc14KzOzCa20gIiIXuACsg/2e4BrI2K9pEslnZVbdQWwMnKHL0fEDuDvyEJmDXBpaivdwrkz2PL4Ph9NbWZTXmm7uQJExGpgdUPbxQ3Llwzx3KuAq0orbghHzZ3Bvp4+tj25n8NnHdLstzczmzAmyiT1hHHSkbMAWP/gEy2uxMystRwQDU48YhYS3LV1V6tLMTNrKQdEg0OntXH0vJms27zz6Vc2M5vEHBAFXnLMPG6+7zGe8tXlzGwKc0AU+NPjO9nX08ea+5uy45SZ2YTkgChw6tHzmNFRZdW6B1tdiplZyzggCkzvqHL2yUfyr3c8yBP7elpdjplZSzgghnDuixbxVE8/19z6h1aXYmbWEg6IIfzR/Nm8/LhOvvzz+3jyKfcizGzqcUAM48NnHM/OvT186aauVpdiZtZ0DohhPG/BbM45ZSH//MtN3LnFR1ab2dTigHgaF515IvMOncaHvruOvd29rS7HzKxpHBBPY/b0dj735hewcdtuPvq9O32WVzObMhwQI3DasZ18+Izj+cG6B/niTza2uhwzs6Yo9XTfk8l7Xv4cNm3fwxd+vJFDp7Xxl6cd3eqSzMxK5YAYoUpFfOpNz2NfTy+f+NE9PL63mw+dfjyVilpdmplZKRwQo9BWrXD5iqXMnr6eK356H13bdvPJNz6fOTM7Wl2amdm48xzEKLVVK/yfN/wRH//zE7npd9tY/sVfcOPdj3jy2swmHQfEGEjiL087mu+/9yXMOqSd//aNtbztq7f6GhJmNqlosnzzXbZsWaxdu7bp79vT18+3b3mAy368kSf29XDK4rm89dRFnH7i4UzvqDa9HjOz0ZB0W0QsK3zMATE+du/v5TtrNnPVf/yerTv3MaOjyqtOPJyXHdfJS455NkfMnt6y2szMhuKAaKK+/uDW3+/gB+u2csPdj7BjTzcAC+dO57lHzOa5R87ihCNmcdTcGSycO50ZHd5PwMxaxwHRIv39we8efpJf3fcov/nDTtY/+AT3P7a3bp15h07jiNmHMHdmB8+e2cHcmR3MPbSD2dPbmdFRZUZHGzM72pjeUWXmtCrT26u0VSu0V0RbtUJbVXRUK7RVRLUiJO92a2YjN1xA+OtriSoVcdKRszjpyFkDbU8+1UPXtt1sfnwfm3fsZfOOvTyy6yke29NN17bd7NjTzb6DuBZ2e1W0VSpZWABSNqkuQUW1tmxZpLZ0f6C9Yd2xGHNMjfGJY32/gwlUR7FNFCccMYsvnbN03F/XAdFkhx3SztKj5rD0qDlDrrO3u5fdT/Wyp7uPPft72deT/bu3u4993X309vfT0xf09vXT2x909/XTm5Z7+rN/+/ohCCIgIgggAvoH7tcey7fl1w36x9i5HGufdKy92TH3gQ+i8xwH82SzcbZwTjlznA6ICWhGR5vnJsys5XwchJmZFXJAmJlZIQeEmZkVckCYmVkhB4SZmRVyQJiZWSEHhJmZFXJAmJlZoUlzLiZJ24EHDuIl5gGPjlM548l1jY7rGh3XNTqTsa5FEdFZ9MCkCYiDJWntUCesaiXXNTqua3Rc1+hMtbo8xGRmZoUcEGZmVsgBccCVrS5gCK5rdFzX6Liu0ZlSdXkOwszMCrkHYWZmhRwQZmZWaMoHhKTlkjZI6pJ0UZPfe6Gkn0q6W9J6Se9L7ZdI2ippXbq9Jvecj6ZaN0h6dYm13S/pzvT+a1PbXEk3StqY/p2T2iXp8lTXHZJeWFJNx+e2yTpJuyS9vxXbS9JVkrZJuivXNurtI+m8tP5GSeeVVNdnJP0uvff3JT0rtS+WtC+33b6ce84fp99/V6r9oK+wOkRto/7djff/2SHq+k6upvslrUvtTdlmw3w2NPdvLLv05NS8AVXgPuBooAP4LXBSE9//COCF6f5hwL3AScAlwIcL1j8p1TgNWJJqr5ZU2/3AvIa2TwMXpfsXAZ9K918D/BvZZZpfDPy6Sb+7h4FFrdhewMuAFwJ3jXX7AHOBTenfOen+nBLqOgNoS/c/latrcX69hte5NdWqVPuZJW2zUf3uyvg/W1RXw+OfAy5u5jYb5rOhqX9jU70HcQrQFRGbIqIbWAmc3aw3j4iHIuL2dP9J4B5g/jBPORtYGRH7I+L3QBfZz9AsZwNfT/e/Drw+1/6NyNwCPEvSESXX8krgvogY7uj50rZXRPwC2FHwfqPZPq8GboyIHRHxOHAjsHy864qIGyKiNy3eAiwY7jVSbbMi4pbIPmW+kftZxrW2YQz1uxv3/7PD1ZV6AW8GrhnuNcZ7mw3z2dDUv7GpHhDzgc255S0M/wFdGkmLgaXAr1PTBamreFWtG0lz6w3gBkm3STo/tR0eEQ+l+w8Dh7egrpoV1P+nbfX2gtFvn1Zst78g+6ZZs0TSbyT9XNJpqW1+qqVZdY3md9fsbXYa8EhEbMy1NXWbNXw2NPVvbKoHxIQg6VDg/wHvj4hdwD8CzwFOBh4i6+I220sj4oXAmcBfSXpZ/sH0Lakl+0hL6gDOAr6bmibC9qrTyu0zFEkfA3qBb6emh4CjImIp8EHgakmzmlzWhPvdNTiH+i8iTd1mBZ8NA5rxNzbVA2IrsDC3vCC1NY2kdrI/gG9HxPcAIuKRiOiLiH7gKxwYFmlavRGxNf27Dfh+quGR2tBR+ndbs+tKzgRuj4hHUo0t317JaLdP0+qT9A7gtcC56YOFNHzzWLp/G9nY/nGphvwwVJl/Z6P93TVzm7UBbwS+k6u3adus6LOBJv+NTfWAWAMcK2lJ+la6AljVrDdP45tfBe6JiM/n2vPj928AantXrAJWSJomaQlwLNnE2HjXNVPSYbX7ZJOcd6X3r+0FcR7wg1xdb097UrwYeCLXDS5D3be6Vm+vnNFun+uBMyTNSUMrZ6S2cSVpOfA/gLMiYm+uvVNSNd0/mmz7bEq17ZL04vQ3+vbczzLetY32d9fM/7OvAn4XEQNDR83aZkN9NtDsv7GxzrJPlhvZ7P+9ZN8EPtbk934pWRfxDmBdur0G+CZwZ2pfBRyRe87HUq0bGIc9S4ao62iyvUN+C6yvbRfg2cBPgI3Aj4G5qV3AFamuO4FlJW6zmcBjwOxcW9O3F1lAPQT0kI3rvmss24dsTqAr3d5ZUl1dZOPQtb+xL6d135R+v+uA24HX5V5nGdmH9X3A35POulBCbaP+3Y33/9miulL714B3N6zblG3G0J8NTf0b86k2zMys0FQfYjIzsyE4IMzMrJADwszMCjkgzMyskAPCzMwKOSDMRkFSn+rPKDtuZwBWdqbQu55+TbPmaGt1AWbPMPsi4uRWF2HWDO5BmI0DZdcM+LSy6wHcKumY1L5Y0k3pZHQ/kXRUaj9c2bUZfptuf5JeqirpK8quAXCDpOkt+6FsynNAmI3O9IYhprfkHnsiIp5HdhTtF1Lbl4CvR8TzyU6Sd3lqvxz4eUS8gOxaBOtT+7HAFRHxXGAn2ZG7Zi3hI6nNRkHS7og4tKD9fuAVEbEpnWTt4Yh4tqRHyU4f0ZPaH4qIeZK2AwsiYn/uNRaTnbv/2LT8N0B7RHyi/J/MbDD3IMzGTwxxfzT25+734XlCayEHhNn4eUvu35vT/V+RnXEU4Fzgl+n+T4D3AEiqSprdrCLNRsrfTsxGZ7rSBeyTf4+I2q6ucyTdQdYLOCe1XQj8i6SPANuBd6b29wFXSnoXWU/hPWRnFDWbMDwHYTYO0hzEsoh4tNW1mI0XDzGZmVkh9yDMzKyQexBmZlbIAWFmZoUcEGZmVsgBYWZmhRwQZmZW6P8DmcId+sPTP8EAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MrC_96GCF82",
        "colab_type": "code",
        "outputId": "ef84d1b4-67a5-42d7-e8b6-d02f2fbbc504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# same dummy data as we used before\n",
        "X = np.array([[1, 1], [1, 0], [0, 1], [0, 0]]).T\n",
        "Y = np.array([0, 1, 1, 0])\n",
        "\n",
        "# number of units in a hidden layer\n",
        "unit_numbers = [1, 2, 3, 4, 5, 10, 20, 50, 100]\n",
        "\n",
        "# try each number of hidden units in the for loop and\n",
        "# print the accuracy for each one\n",
        "# epoch is defined as 1000\n",
        "for n_hidden_units in unit_numbers:\n",
        "\n",
        "  model = MyModel(0.1)\n",
        "\n",
        "  layer_1 = Layer(units=n_hidden_units, input_shape=(2, 1), activation=\"tanh\")\n",
        "  layer_2 = Layer(units=1, activation=\"sigmoid\")\n",
        "\n",
        "  model.add_layer(layer_1)\n",
        "  model.add_layer(layer_2)\n",
        "\n",
        "  model.fit(X, Y, epochs=1000, verbose=False)\n",
        "\n",
        "  predictions = model.predict(X)\n",
        "\n",
        "  accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
        "\n",
        "  print(\"Accuracy for {} hidden units: {}%\".format(n_hidden_units, accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for 1 hidden units: 66.16058159748553%\n",
            "Accuracy for 2 hidden units: 97.02922579728171%\n",
            "Accuracy for 3 hidden units: 96.5205345303777%\n",
            "Accuracy for 4 hidden units: 96.63291050169838%\n",
            "Accuracy for 5 hidden units: 96.49812711822749%\n",
            "Accuracy for 10 hidden units: 96.105484108208%\n",
            "Accuracy for 20 hidden units: 96.60991405851951%\n",
            "Accuracy for 50 hidden units: 96.85797035549724%\n",
            "Accuracy for 100 hidden units: 98.68565859730916%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia7vsOBsjWfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}