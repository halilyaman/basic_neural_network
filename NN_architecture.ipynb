{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_architecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPocfeXoR3ICtyvSZ3J9ytR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halilyaman/neural_network_implementation/blob/master/NN_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1WJ5oHSamXo",
        "colab_type": "text"
      },
      "source": [
        "#### **Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXe8pRunGaiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDKslBsRaxmw",
        "colab_type": "text"
      },
      "source": [
        "# **Activations class**\n",
        "This class contains activation functions and derivatives of them. All functions are static in order to use them directly in a neural network. \\\n",
        "X represents the input matrix. \\\n",
        "\n",
        "---\n",
        " **sigmoid** \\\n",
        "![Sigmoid Function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/400px-Logistic-curve.svg.png)\n",
        "\n",
        "---\n",
        "**tanh** \\\n",
        "![Tanh Function](https://www.medcalc.org/manual/_help/functions/tanh.png)\n",
        "\n",
        "---\n",
        "**ReLU** \\\n",
        "![ReLU Function](https://miro.medium.com/max/400/0*g9ypL5M3k-f7EW85.png)\n",
        "\n",
        "---\n",
        "**Leaky Relu** \\\n",
        "![Leaky ReLU](https://1.bp.blogspot.com/-5ymhxBydo8A/XPj_qXK-sWI/AAAAAAAABU4/UjgZ7eChpwsoPa1_bZjvdrzKCsCfQPaJgCLcBGAs/s400/leaking_relu_2.PNG)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXSoSODaupiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Activations:\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid(X):\n",
        "\n",
        "    return 1 / (1 + np.exp(-X))\n",
        "\n",
        "  @staticmethod\n",
        "  def linear(X):\n",
        "\n",
        "    return X\n",
        "\n",
        "  @staticmethod\n",
        "  def relu(X):\n",
        "\n",
        "    return np.maximum(0, X)\n",
        "\n",
        "  @staticmethod\n",
        "  def leaky_relu(X):\n",
        "\n",
        "    return np.maximum(0.01 * X, X)\n",
        "\n",
        "  @staticmethod\n",
        "  def tanh(X):\n",
        "\n",
        "    return np.tanh(X)\n",
        "\n",
        "  @staticmethod\n",
        "  def sigmoid_derivative(X):\n",
        "\n",
        "    X_sigmoid = Activations.sigmoid(X)\n",
        "\n",
        "    return X_sigmoid * (1 - X_sigmoid)\n",
        "\n",
        "  @staticmethod\n",
        "  def relu_derivative(X):\n",
        "    \n",
        "    derivatives = np.zeros(X.shape)\n",
        "\n",
        "    for i, rows in enumerate(X):\n",
        "\n",
        "      for j, v in enumerate(rows):\n",
        "\n",
        "        if v < 0:\n",
        "          derivatives[i, j] = 0\n",
        "        else:\n",
        "          derivatives[i, j] = 1\n",
        "\n",
        "    return derivatives\n",
        "\n",
        "  @staticmethod\n",
        "  def leaky_relu_derivative(X):\n",
        "    \n",
        "    derivatives = np.zeros(X.shape)\n",
        "\n",
        "    for i, rows in enumerate(X):\n",
        "\n",
        "      for j, v in enumerate(rows):\n",
        "\n",
        "        if v < 0:\n",
        "          derivatives[i, j] = 0.01\n",
        "        else:\n",
        "          derivatives[i, j] = 1\n",
        "\n",
        "    return derivatives\n",
        "\n",
        "  @staticmethod\n",
        "  def tanh_derivative(X):\n",
        "\n",
        "    tanh = Activations.tanh(X)\n",
        "    \n",
        "    return 1 - tanh ** 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1xfiWQjcLJ_",
        "colab_type": "text"
      },
      "source": [
        "# **Layer class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW01W026GtId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer:\n",
        "\n",
        "  def __init__(self, units, input_shape=None, activation=\"linear\"):\n",
        "\n",
        "    self.units = units\n",
        "    self.input_shape = input_shape\n",
        "    self.activation = activation\n",
        "    self.layer_type = None\n",
        "    self.predictions = None\n",
        "    self.inputs = None\n",
        "    self.learning_rate = None\n",
        "    self.next_layer = None\n",
        "    self.activation_derivative = None\n",
        "    self.z_1 = None\n",
        "\n",
        "    if not(self.input_shape == None):\n",
        "      self.__init_weights()\n",
        "  \n",
        "  def __init_weights(self,):\n",
        "\n",
        "    self.w = np.random.rand(self.units, self.input_shape[0])\n",
        "    self.b = np.zeros((self.units, 1))\n",
        "\n",
        "  def _forward_prop(self, X):\n",
        "\n",
        "    if X.shape[0] != self.input_shape[0]:\n",
        "      raise Exception(\"input shape doesn't match with the data!\")\n",
        "\n",
        "    self.inputs = X\n",
        "\n",
        "    dot_products = np.dot(self.w, X)\n",
        "    pred = dot_products + self.b\n",
        "    self.z_1 = pred\n",
        "    self.predictions = self._choose_activation(pred)\n",
        "\n",
        "    return self.predictions\n",
        "  \n",
        "  def _backward_prop(self, Y):\n",
        "\n",
        "    avg_factor = (1 / len(self.inputs))\n",
        "\n",
        "    # derivative calculations are different in order to layer type.\n",
        "    # So we need these conditions\n",
        "    if self.layer_type == \"output_layer\":\n",
        "\n",
        "      # calculating new weights and b values\n",
        "      self.d_z = self.predictions - Y\n",
        "      d_w = avg_factor * self.d_z.dot(self.inputs.T)\n",
        "      d_b = avg_factor * np.sum(self.d_z, axis=1, keepdims=True)\n",
        "      \n",
        "      # updating weights and b\n",
        "      self.b = self.b - self.learning_rate * d_b\n",
        "      self.w = self.w - self.learning_rate * d_w\n",
        "    \n",
        "    if self.layer_type == \"hidden_layer\":\n",
        "\n",
        "      # calculating new weights and b values\n",
        "      self.d_z = self.next_layer.w.T.dot(self.next_layer.d_z) * self.activation_derivative(self.z_1)\n",
        "      d_w = avg_factor * self.d_z.dot(self.inputs.T)\n",
        "      d_b = avg_factor * np.sum(self.d_z, axis=1, keepdims=True)\n",
        "\n",
        "      # updating weights and b\n",
        "      self.b = self.b - self.learning_rate * d_b\n",
        "      self.w = self.w - self.learning_rate * d_w\n",
        "\n",
        "\n",
        "  def _choose_activation(self, X):\n",
        "\n",
        "    if self.activation == \"sigmoid\":\n",
        "      X = Activations.sigmoid(X)\n",
        "      self.activation_derivative = Activations.sigmoid_derivative\n",
        "    \n",
        "    elif self.activation == \"linear\":\n",
        "      X = Activations.linear(X)\n",
        "    \n",
        "    elif self.activation == \"relu\":\n",
        "      X = Activations.relu(X)\n",
        "      self.activation_derivative = Activations.relu_derivative\n",
        "\n",
        "    elif self.activation == \"leaky_relu\":\n",
        "      X = Activations.leaky_relu(X)\n",
        "      self.activation_derivative = Activations.leaky_relu_derivative\n",
        "\n",
        "    elif self.activation == \"tanh\":\n",
        "      X = Activations.tanh(X)\n",
        "      self.activation_derivative = Activations.tanh_derivative\n",
        "\n",
        "    return X\n",
        "    \n",
        "  def _bind_to(self, layer):\n",
        "\n",
        "    self.input_shape = (layer.units, 1)\n",
        "    self.__init_weights()\n",
        "\n",
        "  def _set_layer_type(self, layer_type):\n",
        "\n",
        "    self.layer_type = layer_type\n",
        "\n",
        "  def _set_learning_rate(self, learning_rate):\n",
        "    \n",
        "    self.learning_rate = learning_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rD9tp5FcdwM",
        "colab_type": "text"
      },
      "source": [
        "# **Model class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKi5ojI9zXl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel:\n",
        "  \n",
        "  def __init__(self, learning_rate=0.01):\n",
        "\n",
        "    self.layers = list()\n",
        "    self.learning_rate = learning_rate\n",
        "    self.history = dict()\n",
        "\n",
        "\n",
        "  def add_layer(self, layer):\n",
        "\n",
        "    if len(self.layers) == 0:\n",
        "\n",
        "      self.layers.append(layer)\n",
        "      self.layers[0]._set_layer_type(\"output_layer\")\n",
        "\n",
        "    else:\n",
        "\n",
        "      layer._bind_to(self.layers[-1])\n",
        "      self.layers[-1].next_layer = layer\n",
        "      self.layers.append(layer)\n",
        "      \n",
        "\n",
        "      n = len(self.layers)\n",
        "      last_layer_i = n - 1\n",
        "      before_last_i = n - 2\n",
        "\n",
        "      self.layers[last_layer_i]._set_layer_type(\"output_layer\")\n",
        "      self.layers[before_last_i]._set_layer_type(\"hidden_layer\")\n",
        "\n",
        "    self.layers[-1]._set_learning_rate(self.learning_rate)\n",
        "\n",
        "\n",
        "  def fit(self, X, Y, epochs, verbose=True):\n",
        "\n",
        "    self.history[\"loss\"] = []\n",
        "    m = len(Y)\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "      if verbose:\n",
        "        print(\"Epoch {}\\n\".format(i+1))\n",
        "  \n",
        "      current_output = X\n",
        "      for j in self.layers:\n",
        "\n",
        "        current_output = j._forward_prop(current_output)\n",
        "      \n",
        "      for j in reversed(range(len(self.layers))):\n",
        "\n",
        "        self.layers[j]._backward_prop(Y)\n",
        "      \n",
        "      logprobs = np.multiply(np.log(current_output.flatten()), Y) + np.multiply(np.log(1-current_output.flatten()), (1-Y))\n",
        "      cost = - np.sum(logprobs) / m\n",
        "      \n",
        "      self.history[\"loss\"].append(cost)\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "\n",
        "    current_output = X\n",
        "    for i in self.layers:\n",
        "\n",
        "      current_output = i._forward_prop(current_output)\n",
        "    \n",
        "    return current_output\n",
        "\n",
        "  \n",
        "  def print_layers(self,):\n",
        "    print(\"\\n****************************\\n\")\n",
        "\n",
        "    for i, v in enumerate(self.layers):\n",
        "      print(\"Layer Index: {}\\nLayer Type: {}\\nUnits: {}\\nActivation: {}\\n\"\n",
        "      .format(i, v.layer_type, v.units, v.activation, v.input_size))\n",
        "    \n",
        "    print(\"****************************\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfV9ER-VEAHP",
        "colab_type": "text"
      },
      "source": [
        "# **Testing our neural network model with one hidden layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niVW_wLVQ1nj",
        "colab_type": "code",
        "outputId": "53a55f01-f5a6-4fc7-f5d0-3ae6b82bc14f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "\n",
        "# X and Y are dummy datas which represent the XOR operation.\n",
        "X = np.array([[1, 1], [1, 0], [0, 1], [0, 0]]).T\n",
        "Y = np.array([0, 1, 1, 0])\n",
        "print(X.shape)\n",
        "# get the current time\n",
        "past = time.time()\n",
        "\n",
        "# create the model with learning_rate as a parameter\n",
        "model = MyModel(learning_rate=0.1)\n",
        "\n",
        "# create layers\n",
        "layer_1 = Layer(units=4, input_shape=(2, 1), activation=\"tanh\")\n",
        "layer_2 = Layer(units=1, activation=\"sigmoid\")\n",
        "\n",
        "# add layers to the model\n",
        "model.add_layer(layer_1)\n",
        "model.add_layer(layer_2)\n",
        "\n",
        "# fit the data to the model\n",
        "model.fit(X, Y, 2000, verbose=False)\n",
        "\n",
        "# get prediction after fitting the data\n",
        "# use same data as we used in fitting process\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# calculate the accuracy percentage of the predictions\n",
        "accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
        "\n",
        "print(\"Results after fitting:\\n\", after_fit)\n",
        "print(\"\\nAccuracy: {}%\".format(accuracy))\n",
        "print(\"\\nTime passed: {} seconds\".format(time.time() - past))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 4)\n",
            "Results after fitting:\n",
            " [[0.03386414]\n",
            " [0.97744239]\n",
            " [0.97891174]\n",
            " [0.00428964]]\n",
            "\n",
            "Accuracy: 98.53265227974477%\n",
            "\n",
            "Time passed: 0.147200345993042 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIPH6QVKREPG",
        "colab_type": "code",
        "outputId": "f665c3f0-e60f-45ed-b48f-e9cde1ed05fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.title(\"Loss vs Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.plot(model.history[\"loss\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fee4a04a978>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MrC_96GCF82",
        "colab_type": "code",
        "outputId": "ef84d1b4-67a5-42d7-e8b6-d02f2fbbc504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# same dummy data as we used before\n",
        "X = np.array([[1, 1], [1, 0], [0, 1], [0, 0]]).T\n",
        "Y = np.array([0, 1, 1, 0])\n",
        "\n",
        "# number of units in a hidden layer\n",
        "unit_numbers = [1, 2, 3, 4, 5, 10, 20, 50, 100]\n",
        "\n",
        "# try each number of hidden units in the for loop and\n",
        "# print the accuracy for each one\n",
        "# epoch is defined as 1000\n",
        "for n_hidden_units in unit_numbers:\n",
        "\n",
        "  model = MyModel(0.1)\n",
        "\n",
        "  layer_1 = Layer(units=n_hidden_units, input_shape=(2, 1), activation=\"tanh\")\n",
        "  layer_2 = Layer(units=1, activation=\"sigmoid\")\n",
        "\n",
        "  model.add_layer(layer_1)\n",
        "  model.add_layer(layer_2)\n",
        "\n",
        "  model.fit(X, Y, epochs=1000, verbose=False)\n",
        "\n",
        "  predictions = model.predict(X)\n",
        "\n",
        "  accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
        "\n",
        "  print(\"Accuracy for {} hidden units: {}%\".format(n_hidden_units, accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for 1 hidden units: 66.16058159748553%\n",
            "Accuracy for 2 hidden units: 97.02922579728171%\n",
            "Accuracy for 3 hidden units: 96.5205345303777%\n",
            "Accuracy for 4 hidden units: 96.63291050169838%\n",
            "Accuracy for 5 hidden units: 96.49812711822749%\n",
            "Accuracy for 10 hidden units: 96.105484108208%\n",
            "Accuracy for 20 hidden units: 96.60991405851951%\n",
            "Accuracy for 50 hidden units: 96.85797035549724%\n",
            "Accuracy for 100 hidden units: 98.68565859730916%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia7vsOBsjWfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}